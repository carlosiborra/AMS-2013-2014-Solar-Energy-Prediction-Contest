{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 1. Aprendizaje Automático\n",
    "\n",
    "Authors: Carlos Iborra Llopis (100451170), Alejandra Galán Arrospide (100451273)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Table of contents\n",
    "\n",
    "- [Práctica 1. Aprendizaje Automático](#práctica-1-aprendizaje-automático)\n",
    "  - [0. Table of contents](#0-table-of-contents)\n",
    "  - [1. Requirements](#1-requirements)\n",
    "  - [2. Reading the datasets](#2-reading-the-datasets)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Importing necessary libraries \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "import scipy\n",
    "\n",
    "from matplotlib.cbook import boxplot_stats as bps\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reading the datasets\n",
    "\n",
    "Reading the datasets from the bz2 files, group 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Reading the dataset \"\"\"\n",
    "disp_df = pd.read_csv(\"../data/disp_st2ns1.txt.bz2\",\n",
    "                      compression=\"bz2\", index_col=0)\n",
    "comp_df = pd.read_csv(\"../data/comp_st2ns1.txt.bz2\",\n",
    "                      compression=\"bz2\", index_col=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. EDA\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Concepts of Exploratory Data Analysis**\n",
    "\n",
    "- **2 types of Data Analysis**\n",
    "  - Confirmatory Data Analysis\n",
    "  - Exploratory Data Analysis\n",
    "- **4 Objectives of EDA**\n",
    "  - Discover Patterns\n",
    "  - Spot Anomalies\n",
    "  - Frame Hypothesis\n",
    "  - Check Assumptions\n",
    "- **2 methods for exploration**\n",
    "  - Univariate Analysis\n",
    "  - Bivariate Analysis\n",
    "- **Stuff done during EDA**\n",
    "  - Trends\n",
    "  - Distribution\n",
    "  - Mean\n",
    "  - Median\n",
    "  - Outlier\n",
    "  - Spread measurement (SD)\n",
    "  - Correlations\n",
    "  - Hypothesis testing\n",
    "  - Visual Exploration\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Dataset description\n",
    "\n",
    "- **apcp_sfc**: 3-Hour accumulated precipitation at the surface (kg·m⁽⁻²⁾)\n",
    "- **dlwrf_sfc**: Downward long-wave radiative flux average at the surface (W·m⁽⁻²⁾)\n",
    "- **dswrf_sfc**: Downward short-wave radiative flux average at the surface (W·m⁽⁻²⁾)\n",
    "- **pres_msl**: Air pressure at mean sea level (Pa)\n",
    "- **pwat_eatm**: Precipitable Water over the entire depth of the atmosphere (kg·m⁽⁻²⁾)\n",
    "- **spfh_2m**: Specific Humidity at 2 m above ground (kg·kg⁽⁻¹⁾)\n",
    "- **tcdc_eatm**: Total cloud cover over the entire depth of the atmosphere (%)\n",
    "- **tcolc_eatm**: Total column-integrated condensate over the entire atmos. (kg·m⁽⁻²⁾)\n",
    "- **tmax_2m**: Maximum Temperature over the past 3 hours at 2 m above the ground (K)\n",
    "- **tmin_2m**: Mininmum Temperature over the past 3 hours at 2 m above the ground (K)\n",
    "- **tmp_2m**: Current temperature at 2 m above the ground (K)\n",
    "- **tmp_sfc**: Temperature of the surface (K)\n",
    "- **ulwrf_sfc**: Upward long-wave radiation at the surface (W·m⁽⁻²⁾)\n",
    "- **ulwrf_tatm**: Upward long-wave radiation at the top of the atmosphere (W·m⁽⁻²⁾)\n",
    "- **uswrf_sfc**: Upward short-wave radiation at the surface (W·m⁽⁻²⁾)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df.info()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Missing values\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fist, we check the number the total number of missing values in the dataset in order to know if we have to clean the dataset or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df.isna().sum()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can oberve, there are no missing values in the dataset, but theres still the possibility of having missing values measured as 0's, so we will check if all those zeros make sense in the context of the dataset or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the plot, we can see that there are a lot of 0 values in the dataset\n",
    "disp_df.plot(legend=False, figsize=(15, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = disp_df.eq(0.0).sum()/len(disp_df)*100\n",
    "\n",
    "# Select those columns with more than 30% of zeros\n",
    "result = result[result > 30.0]\n",
    "result = result.sort_values(ascending=False)\n",
    "result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "As output of the previous cell, we can see that there exist a lot of zeros in the dataset, let's analize if those zeros make sense or not.\n",
    "\n",
    "The variables with most ammount of zeros (>30%) are:\n",
    "\n",
    "- **dswrf_s1_1**: Downward short-wave radiative flux average at the surface, at 12:00 UTC, normal to have a lot of zeros as it is not sunny at 12:00\n",
    "- **uswrf_s1_1**: Upward short-wave radiation at the surface, at 12:00 UTC, normal to have a lot of zeros as it is not sunny at 12:00\n",
    "- **apcp_s**: 3-Hour accumulated precipitation at the surface, as it is not raining every day, it is normal to have a lot of zeros\n",
    "- **tcdc_ea**: Total cloud cover over the entire depth of the atmosphere, as it is not cloudy every day, it is normal to have a lot of zeros\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets start by assigning the zeros to NaNs. By doing this we can visualize the varibles that take more values other than zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df_nan = disp_df.replace(0.0, np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Plotting missing values \"\"\"\n",
    "# Sustitute 0.0 values with NaN and plot the name of the columns with missing values\n",
    "# ? msno.bar is a simple visualization of nullity by column\n",
    "msno.bar(disp_df_nan, labels=True, fontsize=7, figsize=(15, 7))\n",
    "\n",
    "# Exporting image as png to ../data/img folder\n",
    "plt.savefig(\"../data/img/missing_values_bar.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Plotting the missing values in a matrix \"\"\"\n",
    "# ? The msno.matrix nullity matrix is a data-dense display which lets you quickly visually pick out patterns in data completion.\n",
    "msno.matrix(disp_df_nan, labels=True, fontsize=7, figsize=(15, 7), color=(.3, .3, .3))\n",
    "\n",
    "# Exporting image as png to ../data/img folder\n",
    "plt.savefig(\"../data/img/missing_values_matrix.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Plotting the missing values in a heatmap \"\"\"\n",
    "# As in a hetmap not every value is shown, we must delimit the values to the ones with more than 30% of missing values\n",
    "result = disp_df.eq(0.0).sum()/len(disp_df)*100\n",
    "result = result[result > 30.0] # Select those columns with more than 30% of zeros\n",
    "result = result.sort_values(ascending=False)\n",
    "result = result.index.tolist() # Convert to list\n",
    "result\n",
    "\n",
    "# ? The missingno correlation heatmap measures nullity correlation: how strongly the presence or absence of one variable affects the presence of another\n",
    "msno.heatmap(disp_df_nan[result], fontsize=7, figsize=(15, 7))\n",
    "\n",
    "# Exporting image as png to ../data/img folder\n",
    "plt.savefig(\"../data/img/missing_values_heatmap.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Plotting the dendrogram \"\"\"\n",
    "\n",
    "# ? The dendrogram allows you to more fully correlate variable completion, revealing trends deeper than the pairwise ones visible in the correlation heatmap:\n",
    "msno.dendrogram(disp_df_nan, orientation=\"top\",fontsize=7, figsize=(15, 7))\n",
    "\n",
    "# Exporting image as png to ../data/img folder\n",
    "plt.savefig(\"../data/img/missing_values_dendrogram.png\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "### Conclusions\n",
    "In this section, we have observe that there are no attibutes with 'Null' nor 'NaN' nor 'None' values. This indicated that at a first glance, the data is clean, at least of those datatypes.\n",
    "\n",
    "In second place, we have observed that the attributes that we suspected could have an important number of missing values (represented by 0 instead of the previously mentioned), had instead valuable information, as we have proved along this section.<br>Since the data is clean and we have concluded there are no missing values, we do not need to complete them using a model or other methods, so we can move on to the next step, observing the outliers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Outliers\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the objective of noticing the outliers on each attribute, we create a box-plot of each of the attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_attributes = disp_df.columns.values.tolist()\n",
    "#print(list_of_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boxplot with all attributes in the dataset\n",
    "#sns.boxplot(data=disp_df, orient=\"h\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## disp_df['apcp_sf1_1'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, by plotting the boxplots and making the outliers (fliers) visible, we are able to see some outliers in the dataset.<br>\n",
    "Take into account that the outliers are represented by the points outside the boxplot and they can be potentially wrong values or just values that are not usual in the dataset (ruido)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Plotting the boxplot for each attribute and getting the outliers of each attribute \"\"\"\n",
    "total_outliers = []\n",
    "# * We iterate over the list of attributes\n",
    "for attribute in list_of_attributes:\n",
    "    # * sns.regplot(x=disp_df[attribute], y=disp_df['total'], fit_reg=False)\n",
    "    sns.boxplot(data=disp_df[attribute], x=disp_df[attribute], orient=\"h\")\n",
    "    # * Use the command below to show each plot (small size for visualization sake)\n",
    "    # sns.set(rc={'figure.figsize':(1,.5)})\n",
    "    # plt.show()\n",
    "    # * All the images are saved in the folder ../data/img/box-plot\n",
    "    plt.savefig(f\"../data/img/box-plot/{str(attribute)}.png\")\n",
    "\n",
    "    # We obtain the a list of outliers for each attribute\n",
    "    list_of_outliers = disp_df[attribute][disp_df[attribute] > disp_df[attribute].quantile(0.75) + 1.5*(disp_df[attribute].quantile(0.75) - disp_df[attribute].quantile(0.25))].tolist()\n",
    "    outliers = [f'{attribute} outliers'] + [len(list_of_outliers)] + [list_of_outliers]\n",
    "    # * In orde to print the total number of outliers for each attribute\n",
    "    # print(f'{attribute} has {len(list_of_outliers)} outliers')\n",
    "    # ! Data structure: [attribute, number of outliers, list of outliers]\n",
    "    print(outliers)\n",
    "    total_outliers.append(outliers)\n",
    "\n",
    "print(total_outliers)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed to create a list containing the name of the atribute, the number of outliers and the value of the outliers for each attribute, calculated by applying the IQR method.\n",
    "\n",
    "As suspected, we can see that there are a lot of outliers in the dataset, therefore it is plausible that some of them are noise, thus possibly being removed in a future model in order to improve it.<br>Now, we need to analyze if they are the result of bad measurements or if they are significant data for the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Skewness \"\"\"\n",
    "# ? skewness: measure of the asymmetry of the probability distribution of a real-valued random variable about its mean.\n",
    "disp_df.skew().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Kurtosis \"\"\"\n",
    "# ? kurtosis: measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution.\n",
    "disp_df.kurt().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = disp_df['apcp_sf4_1']\n",
    "plt.figure(1); plt.title('Normal')\n",
    "sns.distplot(y, kde=True, fit=st.norm)\n",
    "plt.figure(2); plt.title('Log Normal')\n",
    "sns.distplot(y, kde=True, fit=st.lognorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(disp_df.skew(),color='blue',axlabel ='Skewness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "sns.distplot(disp_df.kurt(),color='r',axlabel ='Kurtosis',norm_hist= False, kde = True,rug = False)\n",
    "#plt.hist(train.kurt(),orientation = 'vertical',histtype = 'bar',label ='Kurtosis', color ='blue')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = disp_df.corr()\n",
    "correlation = abs(correlation)\n",
    "print(correlation)  # 76 x 76 matrix of correlation values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the correlation matrix formatted into our own data structure\n",
    "This is done for the sake of simplicity and to be able to visualize the correlation matrix in a more intuitive way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_list = []\n",
    "\n",
    "for column in disp_df:\n",
    "    correlation[column] = abs(correlation[column])\n",
    "    mask = correlation[column] > 0.95\n",
    "    # print(correlation[column][mask].sort_values(ascending = False))\n",
    "    \n",
    "    # Translate the comment below to English: \n",
    "    # we add the correlation values to a list of lists, which contains the names of the correlated columns and their correlation index\n",
    "    \n",
    "    # The first segment adds the name of the column we are analyzing\n",
    "    # The second segment adds the names of the columns correlated (except the column we are analyzing) > 0.95\n",
    "    # The third segment adds the correlation index of the columns correlated (except the column we are analyzing) > 0.95\n",
    "    # Second and third segment are added to the first segment as a list of lists\n",
    "    \n",
    "    # First we need to create a dictionary with the column names and their correlation values (except the column we are analyzing)\n",
    "    dict = ({key: value for key, value in correlation[column][mask].sort_values(ascending = False).to_dict().items() if key != column})\n",
    "    # print (dict)\n",
    "    \n",
    "    # Then we create a list of lists with the column names and their correlation values from the dictionary created above\n",
    "    corr_list = [[key] + [value] for key, value in dict.items()]\n",
    "    # Finally we add the name of the column we are analyzing to the list of lists created above as the first element of the list (str)\n",
    "    corr_list.insert(0, f\"Columna: {column}\")\n",
    "    \n",
    "    # ! Data structure: [[columna, [columna correlada 1, indice de correlacion], [columna correlada 2, indice de correlacion], ...], ...] \n",
    "    print(corr_list)\n",
    "    \n",
    "    correlation_list += [corr_list]\n",
    "print(correlation_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Heat Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" seaborne Correlation Heat Map \"\"\"\n",
    "\n",
    "# It needs to show all the columns\n",
    "fig, ax = plt.subplots(figsize=(19,18))\n",
    "\n",
    "plt.title('Correlation Heat Map',y=1)\n",
    "# We use blue color scale because it is easier to see the annotations and the correlation values\n",
    "sns.heatmap(correlation, square=True, cmap='Blues', annot=True, fmt='.2f', annot_kws={'size': 4}, linewidths=.3, cbar_kws={\"shrink\": .5}, vmin=0.0, vmax=1)\n",
    "# We can modify vmax=0.95 in order to get same color scale for values with more than 0.95 correlation\n",
    "# Note: it delays around 15 seconds as it needs to plot a 76*76 matrix with its 5766 correlation values\n",
    "\n",
    "# Exporting image as png to ../data/img folder - easier to visualize the annotations, better resolution\n",
    "plt.savefig(\"../data/img/correlation_heatmap.png\", dpi=200)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once obtained the most correlated columns of the dataset, we can plot them and visualize their correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "columns = []\n",
    "\n",
    "# We get all the columns -- lot of time (76x76)\n",
    "# for column in disp_df:\n",
    "#     columns.append(column)\n",
    "# print(columns)\n",
    "\n",
    "columns = ['apcp_sf1_1', 'apcp_sf5_1', 'dlwrf_s1_1', 'dlwrf_s2_1', 'dlwrf_s3_1', 'dlwrf_s4_1', 'dlwrf_s5_1', 'dswrf_s1_1', 'dswrf_s2_1', 'dswrf_s3_1', 'dswrf_s4_1']\n",
    "\n",
    "sns.pairplot(disp_df[columns], size = 1 ,kind ='scatter',diag_kind='kde')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_attributes = disp_df.columns.values.tolist()\n",
    "tuplas_correlacion = []\n",
    "for column in correlation: \n",
    "    for value in correlation[column][mask].sort_values(ascending = False):\n",
    "        if value > 0.95:\n",
    "            locals()[column].append(correlation[column][mask].sort_values(ascending = False).head(5).index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = disp_df.corr()\n",
    "correlation = abs(correlation)\n",
    "for column in disp_df:\n",
    "    \n",
    "    mask = correlation[column] > 0.9\n",
    "    correlation = correlation[column][mask]\n",
    "    print(correlation)\n",
    "    #print(correlation[column].sort_values(ascending = False).head(5),'\\n')\n",
    "    for i in correlation[column]: \n",
    "        locals()[\"index\" + str(column)]  = pd.array().append(correlation[column].find(correlation[column][i]))\n",
    "        #We add all the indexes of the correlation that are above 0.9\n",
    "    locals()[\"index\" + str(column)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f , ax = plt.subplots(figsize = (14,12))\n",
    "plt.title('Correlation of Numeric Features with Sale Price',y=1,size=16)\n",
    "sns.heatmap(correlation,square = True,  vmax=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k= 11\n",
    "cols = correlation.nsmallest(k, 'salida')['salida'].index\n",
    "print(cols)\n",
    "cm = np.corrcoef(disp_df[cols].values.T)\n",
    "f , ax = plt.subplots(figsize = (14,12))\n",
    "sns.heatmap(cm, vmax=.8, linewidths=0.01,square=True,annot=True,cmap='viridis',\n",
    "            linecolor=\"white\",xticklabels = cols.values ,annot_kws = {'size':12},yticklabels = cols.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4c01c8f52586033bb4bb2bf4f864991e275e728809d596b5bf2a750c26b84f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
