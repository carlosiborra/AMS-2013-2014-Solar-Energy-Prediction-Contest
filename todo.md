# TODO

## Lo que se nos pide

1) (0.5 puntos) <del>Preparar un repositorio privado en GitHub para poder hacer los commits semanales de lo realizado en la prÃ¡ctica cada semana. Haciendo al menos un commit cada semana se obtienen 0.5 puntos. Se recomienda que el nombre del repositorio sea vuestro nÃºmero de grupo de prÃ¡cticas seguido con el literal â€œPractica1â€. Por ejemplo, si sois el grupo 13 de prÃ¡cticas, el repositorio se llamarÃ¡ â€œGrupo13-Practica1â€. Enviar el enlace del repositorio al profesor de prÃ¡cticas por e-mail.</del>
2) <del>Leer los conjuntos de datos. Cada grupo usarÃ¡ una planta solar distinta (punto rojo), sustituyendo xx por el nÃºmero de grupo</del>
3) (0.25 puntos) <del>Hacer un AnÃ¡lisis Exploratorio de Datos (EDA).</del>
4) <del>Dividir los datos en Â«trainÂ» (los 10 primeros aÃ±os) y Â«testÂ» (los 2 Ãºltimos). Siguiendo la metodologÃ­a planteada en la competiciÃ³n, no desordenaremos los datos antes de partir en entrenamiento y test, sino que respetaremos el orden temporal. <mark>Importante: los datos de Â«testÂ» se reservan para la evaluaciÃ³n final en el apartado 8 de la prÃ¡ctica, no se puede utilizar para tomar decisiones durante el resto de puntos de la prÃ¡ctica (es decir, desde los apartados 4 a 7, habrÃ¡ que evaluar los distintos mÃ©todos sin usar dicho conjunto de test).</mark></del>
5) (1.00 puntos) <del>MÃ©todos bÃ¡sicos: aquÃ­ se considerarÃ¡n los siguientes mÃ©todos bÃ¡sicos: KNN, Ã¡rboles de regresiÃ³n, regresiÃ³n lineal. Las mÃ©tricas de evaluaciÃ³n son RMSE y MAE. Aparte de evaluar las mÃ©tricas, tambiÃ©n se medirÃ¡ el tiempo que tarda su entrenamiento.</del>
   1) ğŸš¨ Se evaluarÃ¡n dichos modelos con sus hiperparÃ¡metros por omisiÃ³n.
   2) ğŸš¨ DespuÃ©s, se ajustarÃ¡n los hiperparÃ¡metros mÃ¡s importantes de cada mÃ©todo y se obtendrÃ¡ su evaluaciÃ³n.
   3) ğŸš¨ Obtener algunas conclusiones, tales como: Â¿cuÃ¡l es el mejor mÃ©todo? Â¿CuÃ¡l de los mÃ©todos bÃ¡sicos de aprendizaje automÃ¡tico es mÃ¡s rÃ¡pido? Â¿Los resultados son mejores que los regresores triviales/naive/baseline? Â¿El ajuste de hiperparÃ¡metros mejora con respecto a los valores por omisiÃ³n? Â¿Hay algÃºn equilibrio entre tiempo de ejecuciÃ³n y mejora de resultados? Etc.
6) (0.75 puntos) ğŸš¨ Â¿Es posible reducir la dimensionalidad del problema? (aquÃ­ no tiene por quÃ© utilizarse una tÃ©cnica estÃ¡ndar, sino algo que se os ocurra para que en los datos haya menos atributos sin empeorar resultados).
7) (0.75 puntos) MÃ©todos avanzados: SVMs, Random Forests.
   1) ğŸš¨ Se evaluarÃ¡n dichos modelos con sus hiperparÃ¡metros por omisiÃ³n.
   2) ğŸš¨ DespuÃ©s, se ajustarÃ¡n los hiperparÃ¡metros mÃ¡s importantes de cada mÃ©todo y se obtendrÃ¡ su evaluaciÃ³n.
   3) ğŸš¨ Interpretar la importancia de los atributos segÃºn aquellas tÃ©cnicas que lo permitan.
   4) ğŸš¨ Conclusiones hasta el momento.
8) (0.25 puntos) ğŸš¨ Seleccionar el mejor mÃ©todo, evaluarlo, construir modelo final, hacer predicciones para la competiciÃ³n.
   1) Seleccionar el mejor mÃ©todo de los evaluados en los puntos anteriores.
   2) Usar la particiÃ³n de test para evaluar ese mejor mÃ©todo. Esta es una estimaciÃ³n de cÃ³mo se desempeÃ±arÃ­a el modelo en la competiciÃ³n.
   3) Entrenar el modelo final. Guardarlo en un fichero (llamado Â«modelo_final.pklÂ»).
   4) Utilizar el modelo final para obtener predicciones para el conjunto de datos de la competiciÃ³n (comp). Guardar estas predicciones en un fichero (llamado Â«predicciones.csvÂ»)

## Por hacer / completar

- Repasar todos los modelos
- Argumentar el porq se ha usado nMAE como criterion
- Explicar porq se usara el criterion para seleccionar el mejor modelo (al hacerse con validacion)
- Explicar lo de sobreajuste y como se sabe
- Explicacion de que modelo se escoge
- Entrenar previamente el modelo seleccionado con todos los datos antes de pasarlo a formarto pkl
- Explicar porq se usa un scatterplot
- Cambiar tamaÃ±o texto de los histogramas con los resultados

## A preguntar

<!-- - Escalado y eliminar param. tanto en por defecto como no -->
<!-- - Seleccion de param, cuando y porq no antes de los sencillos -->
<!-- - Hacer test a parte de train para comparar valores -->
<!-- - Porq con ajuste de hp escalado y tal funciona tan lento -->
- Porq SVM's por omision es una linea recta -> HAY QUE ESCALAR
- Hacer conclusiones y todo eso en espaÃ±ol -> REPASAR EL HECHO DE QUE SEA UNA LINEA RECTA
<!-- - Si no se puede comprobar los resultados de test, Â¿cÃ³mo se puede comprobar que se ha hecho bien?, que decisiones puedo tomar en los apartados 4-7, si no poseo esa informaciÃ³n -->
- Que hago para dividir los datos y eliminar variables
- El escalado robusto estÃ¡ bien?
- Â¿QuÃ© mÃ¡s podrÃ­amos probar/hacer de los modelos?
- Â¿Resultados OK?
- GRadient Boosting lo aÃ±adimos o no?
- No entiendo porq en SVR si lo cambio de 10 a 15 de budget tarda demasiado
- Se puede utilizar test en el apartado 8 para comparar todos?
- En randForest puedo poner solo log2 ya que es el que mejor funciona?
- Porq NMAE y MAE son distintos, bastatne, y al mae de train detecta el sobreeajuste
- Lo mismo se debe subir el numero de budget para que sea igual en todos y el tiempo sea comparable, o dividir el tiempo entre el nÃºmero de folds
- El modelo final, tras la seleccion de parametros, hay que seleccionar los atributos?